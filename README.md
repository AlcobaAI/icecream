# ğŸŒ IceCream - Modular Web Scraper

A robust, configurable, and modular web scraping framework for crawling and extracting structured content from websites. Supports translation-aware scraping and customizable element filtering.

---

## ğŸš€ Features

- âœ… **Modular Design** â€” Plug-and-play architecture for custom scraper modules
- ğŸ“ **Config-Driven** â€” Define scraping behavior through JSON configuration
- ğŸ” **Checkpointing** â€” Automatically saves progress to resume interrupted runs
- ğŸ” **Selective Extraction** â€” Finds content with flexible tag-based filters
- ğŸ› ï¸ **CLI Interface** â€” Fully scriptable for automation

---

## ğŸ—‚ï¸ Project Structure

```bash
.
â”œâ”€â”€ scrape.py               # Main entry point for running scrapers
â”œâ”€â”€ scrape_utils.py         # Utilities for HTTP requests and HTML parsing
â”œâ”€â”€ init_config.py          # Script to generate new config files
â”œâ”€â”€ config/                 # Stores JSON configuration files
â”œâ”€â”€ data/                   # Output directory for JSONL data
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ base_module.py      # Generic scraping logic
â”‚   â””â”€â”€ translation.py      # Extension for dual-language scraping

---
```

## âš™ï¸ How to Use
1ï¸âƒ£ Create a Config File
```bash
python init_config.py myproject
```

2ï¸âƒ£ Run the Scraper
```bash
python scrape.py --config_file config/myproject_config.json
```
To restart and clear previous progress:

```bash
python scrape.py --config_file config/myproject_config.json --reset
```

## ğŸ”§ Configuration Guide
A sample config/example_config.json file:

```bash
{
  "filename": "example",
  "module_name": "modules.base_module",
  "common_url": "https://example.com",
  "avoid": ["logout", "signup"],
  "filter": ["news", "article"],
  "search_contents": [
    {"tag": "div", "class": "main-content"}
  ],
  "search_elements": [
    {"tag": "p"}
  ],
  "urls": ["https://example.com/start"],
  "seen": []
}
```

## ğŸŒ Translation Module Example
Set "module_name": "modules.translation" and add:

```bash
{
  "en_lang_code": "/en/",
  "ar_lang_code": "/ar/",
  "category": "news"
}
```
This will scrape Arabic and English versions of the same page and pair them.

## ğŸ§  Output Format
Saved as data/<filename>.jsonl (one JSON object per line):

Base Module:
```bash
{
  "url": "https://example.com/page",
  "text": {
    "0 - p": "Some paragraph text",
    "1 - h2": "Section heading"
  }
}
```
Translation Module:

```bash
{
  "url": "https://example.com/ar/page",
  "text_ar": { ... },
  "text_en": { ... },
  "category": "news"
}
```
## ğŸ”Œ Creating Custom Scraper Modules
You can extend the base scraper by writing your own in modules/. 

